{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4282857, 15)\n",
      "(1573876, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>gender</th>\n",
       "      <th>birthday</th>\n",
       "      <th>age</th>\n",
       "      <th>relationship_status</th>\n",
       "      <th>interested_in</th>\n",
       "      <th>mf_relationship</th>\n",
       "      <th>mf_dating</th>\n",
       "      <th>mf_random</th>\n",
       "      <th>mf_friendship</th>\n",
       "      <th>mf_whatever</th>\n",
       "      <th>mf_networking</th>\n",
       "      <th>locale</th>\n",
       "      <th>network_size</th>\n",
       "      <th>timezone</th>\n",
       "      <th>target_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>631f821671a94f991413e58c2b393b29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1982-08-09</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bdeb2156021c5da9237f5d2667ab590a</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1983-07-26</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_US</td>\n",
       "      <td>681.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>d9fb05627d4659484703e5827690b9f9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1985-01-21</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_US</td>\n",
       "      <td>586.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>adc399722fccda28049cb26c5a795b16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1983-12-19</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_US</td>\n",
       "      <td>300.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>18f92f9c1f86b5cd692e8fcc19721b1d</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1985-02-19</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_US</td>\n",
       "      <td>532.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              userid  gender    birthday   age  \\\n",
       "6   631f821671a94f991413e58c2b393b29     1.0  1982-08-09  30.0   \n",
       "7   bdeb2156021c5da9237f5d2667ab590a     1.0  1983-07-26  29.0   \n",
       "12  d9fb05627d4659484703e5827690b9f9     0.0  1985-01-21  27.0   \n",
       "23  adc399722fccda28049cb26c5a795b16     1.0  1983-12-19  29.0   \n",
       "29  18f92f9c1f86b5cd692e8fcc19721b1d     1.0  1985-02-19  27.0   \n",
       "\n",
       "    relationship_status  interested_in  mf_relationship  mf_dating  mf_random  \\\n",
       "6                   3.0            NaN              NaN        NaN        NaN   \n",
       "7                   2.0            NaN              NaN        NaN        NaN   \n",
       "12                  NaN            NaN              NaN        NaN        NaN   \n",
       "23                  2.0            1.0              NaN        NaN        NaN   \n",
       "29                  1.0            1.0              NaN        NaN        NaN   \n",
       "\n",
       "    mf_friendship  mf_whatever  mf_networking locale  network_size  timezone  \\\n",
       "6             NaN          NaN            NaN  en_US           NaN       NaN   \n",
       "7             NaN          NaN            NaN  en_US         681.0      -5.0   \n",
       "12            NaN          NaN            NaN  en_US         586.0      -4.0   \n",
       "23            1.0          NaN            NaN  en_US         300.0      -5.0   \n",
       "29            NaN          NaN            NaN  en_US         532.0      -7.0   \n",
       "\n",
       "    target_age  \n",
       "6        False  \n",
       "7        False  \n",
       "12       False  \n",
       "23       False  \n",
       "29       False  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# Prepare people data frame\n",
    "df_people = pd.read_csv('Data/demog.csv')\n",
    "print(df_people.shape)\n",
    "# Keep only people for which we have age\n",
    "df_people = df_people[~pd.isna(df_people.age)]\n",
    "print(df_people.shape)\n",
    "# Target teenagers\n",
    "df_people['target_age'] = df_people.age >= 50\n",
    "#df_people = df_people[['userid', 'target_age', 'age']]\n",
    "df_people.userid = df_people.userid.astype('category')\n",
    "df_people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55775701, 2)\n"
     ]
    }
   ],
   "source": [
    "# Prepare likes data frame\n",
    "df_likes = pd.read_csv('Data/likes.csv', header=None, names=['userid', 'like_id'])\n",
    "df_likes.userid = df_likes.userid.astype('category')\n",
    "print(df_likes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes before and after\n",
      "(55775701, 2)\n",
      "(37686388, 2)\n",
      "People before and after\n",
      "(1573876, 16)\n",
      "(588610, 16)\n",
      "Pages before and after\n",
      "15914\n",
      "10822\n",
      "Likes before and after\n",
      "(33873012, 2)\n",
      "(33873012, 2)\n",
      "People before and after\n",
      "(588610, 16)\n",
      "(587745, 16)\n",
      "Pages before and after\n",
      "10822\n",
      "10822\n",
      "Likes before and after\n",
      "(33873012, 2)\n",
      "(33873012, 2)\n",
      "People before and after\n",
      "(587745, 16)\n",
      "(587745, 16)\n",
      "Pages before and after\n",
      "10822\n",
      "10822\n"
     ]
    }
   ],
   "source": [
    "# Prune to keep only pages with more than 1,000 users and users with at least one like\n",
    "change = True\n",
    "while change:\n",
    "    # Keep only likes of people for which we have information\n",
    "    print(\"Likes before and after\")\n",
    "    val = df_likes.shape[0]\n",
    "    print(df_likes.shape)\n",
    "    df_likes = df_likes[df_likes.userid.isin(df_people.userid)]\n",
    "    print(df_likes.shape)\n",
    "    change = val != df_likes.shape[0]\n",
    "\n",
    "    # Keep only people with at least one like\n",
    "    print(\"People before and after\")\n",
    "    val = df_people.shape[0]\n",
    "    print(df_people.shape)\n",
    "    df_people = df_people[df_people.userid.isin(df_likes.userid.unique())]\n",
    "    print(df_people.shape)\n",
    "    change = (val != df_people.shape[0]) | change\n",
    "    # Keep only pages with at least 1,000 likes\n",
    "    print(\"Pages before and after\")\n",
    "    counts = df_likes.like_id.value_counts()\n",
    "    val = len(counts)\n",
    "    high_counts = counts[counts >= 1000]\n",
    "    change = (val != len(high_counts)) | change\n",
    "    print(len(counts))\n",
    "    df_likes = df_likes[df_likes.like_id.isin(high_counts.index.values)]\n",
    "    print(len(high_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<587745x10822 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 33873012 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get labels\n",
    "labels = df_people.set_index('userid').loc[sorted(df_likes.userid.unique())].target_age.values\n",
    "ages = df_people.set_index('userid').loc[sorted(df_likes.userid.unique())].age.values\n",
    "# Build sparse matrix\n",
    "users_c = np.array(sorted(df_likes.userid.unique()))\n",
    "pages_c = np.array(sorted(df_likes.like_id.unique()))\n",
    "row = pd.Categorical(df_likes.userid, categories=users_c, ordered=True).codes\n",
    "col = pd.Categorical(df_likes.like_id, categories=pages_c, ordered=True).codes\n",
    "sparse_mat = csr_matrix((np.ones(df_likes.shape[0]), (row, col)), shape=(users_c.size, pages_c.size))\n",
    "sparse_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferlo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8574013931369411"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build machine learning model\n",
    "X_train, X_test, y_train, y_test, age_train, age_test = train_test_split(sparse_mat, labels, ages,\n",
    "                                                                         test_size=0.3, random_state=42)\n",
    "\n",
    "model = LogisticRegression(random_state=42, solver='lbfgs').fit(X_train, y_train)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19041200, 2)\n",
      "(10821, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9254817079</th>\n",
       "      <td>photography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103144503058854</th>\n",
       "      <td>Poetry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104046862964234</th>\n",
       "      <td>Writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105667052799444</th>\n",
       "      <td>Piano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106059522759137</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name\n",
       "like_id                     \n",
       "9254817079       photography\n",
       "103144503058854       Poetry\n",
       "104046862964234      Writing\n",
       "105667052799444        Piano\n",
       "106059522759137      English"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load names of facebook pages\n",
    "df_names = pd.read_csv('Data/fb_like.csv', encoding='latin-1', error_bad_lines=False, warn_bad_lines=False)\n",
    "print(df_names.shape)\n",
    "df_names.head()\n",
    "df_names = df_names[df_names.like_id.isin(pages_c)]\n",
    "print(df_names.shape)\n",
    "df_names = df_names.set_index('like_id')\n",
    "df_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To target top 1%, target individuals with more than:  0.41116640934620974\n",
      "AVG number of likes of targeted people: 73.29024943310658\n",
      "Median number of likes of targeted people: 41.0\n",
      "Misclassified individuals between 30 and 40 years: 165\n",
      "Probabilities of misclassified individuals:\n",
      "[0.84565248 0.74256652 0.78119628 0.72119612 0.41385715 0.68499266\n",
      " 0.45898055 0.98829092 0.80072783 0.43934153 0.81048513 0.73170057\n",
      " 0.58773163 0.41981386 0.74491212 0.55494788 0.47974449 0.50910381\n",
      " 0.47437852 0.58547422 0.53424198 0.49039252 0.69717416 0.8333557\n",
      " 0.75581428 0.44579571 0.56518143 0.92140531 0.41324849 0.42370999\n",
      " 0.461326   0.48154155 0.74343522 0.57035595 0.84301434 0.51596466\n",
      " 0.57772989 0.60031359 0.42048403 0.41358187 0.44502888 0.62397304\n",
      " 0.65683409 0.86094985 0.68844824 0.41694971 0.99615006 0.77394067\n",
      " 0.5744485  0.76198515 0.5085968  0.5547998  0.76078767 0.51821855\n",
      " 0.53853298 0.75322157 0.74039866 0.88581494 0.49459344 0.98338893\n",
      " 0.46801608 0.6121224  0.98678264 0.81081969 0.46479025 0.53228398\n",
      " 0.49764876 0.45280455 0.71262847 0.56081876 0.83838766 0.43101128\n",
      " 0.44395183 0.53432619 0.42486273 0.72832017 0.43929659 0.72226813\n",
      " 0.62772289 0.46752633 0.60252258 0.44827834 0.59651308 0.52774471\n",
      " 0.74344659 0.62804063 0.42923071 0.7033097  0.51960663 0.57661088\n",
      " 0.95487656 0.61944583 0.44949787 0.74816945 0.46163905 0.47435035\n",
      " 0.48851351 0.53384805 0.44370143 0.67661798 0.45601022 0.56547261\n",
      " 0.41256888 0.93148467 0.59401075 0.86260373 0.50588151 0.99894016\n",
      " 0.70984723 0.48870495 0.80834661 0.4765172  0.44952447 0.4352377\n",
      " 0.69316842 0.47920668 0.68288012 0.98833299 0.62306843 0.75223188\n",
      " 0.51143776 0.70601149 0.4349285  0.41962539 0.70938569 0.80636672\n",
      " 0.54377788 0.41263478 0.52494796 0.77585701 0.48572377 0.42289048\n",
      " 0.5884242  0.80294896 0.88478707 0.45423748 0.43246945 0.51623941\n",
      " 0.53933517 0.9171079  0.68830751 0.48810815 0.45494344 0.91067952\n",
      " 0.46484728 0.95256013 0.49704553 0.58804749 0.61092006 0.93750896\n",
      " 0.63814062 0.52219683 0.4424392  0.42047279 0.46001962 0.43541836\n",
      " 0.65646352 0.4445853  0.43545688 0.43439848 0.58455317 0.61785209\n",
      " 0.50370442 0.60835483 0.42032382]\n",
      "Number of likes of misclassified individuals:\n",
      "[[ 94.  66.  37. 125.  22.  64.  19.  44.  95.  23.  42.  38.  31.  15.\n",
      "   12.  23.   8.  59.  59.  12.  57.  62.  12.  37.  50.  27.  62. 115.\n",
      "   19.  25.  42.  12. 107.  34.  59.  13.  24.  95.  39.   8.  40.  32.\n",
      "   47.  14.  32.  46.  66.  45.  27.  85.  24.  26.  92.  17.  23.  44.\n",
      "   36.  20.  33. 167.  26.  31. 104.  26. 114.  41.  25.  14.  23.  69.\n",
      "  146.  28.   9.  16.  11.  38.  14.  23.  53.  38.  12.   9.  34.  40.\n",
      "  123. 158. 150.  23.  50.   9.  35.  38.  16.  87.  41.  20.  24. 375.\n",
      "   40. 133.  27.  13.  71.  61.  55. 164.  12. 123.  59.  17.  28.  31.\n",
      "   69.  20.  65.  69.  26.  61. 265.  39.   8.  45.  35.  64.  45.  35.\n",
      "   27.  53.  18. 157.  40.  28.  41.  51.  53.  21.  22. 137.  45.  36.\n",
      "   30.  36.  20.  54.  63.  48.  14.  11.  80. 224.  27.  24.  69.  12.\n",
      "   10.  95.  90.  65.  16. 129.  19.  15.  52.  35.  14.]]\n"
     ]
    }
   ],
   "source": [
    "# Get threshold\n",
    "probs = model.predict_proba(X_test)[:, 1]\n",
    "threshold = np.percentile(probs, 99)\n",
    "print(\"To target top 1%, target individuals with more than: \", threshold)\n",
    "# Keep only people with probabilities above threshold\n",
    "top_x = X_test[probs > threshold, :]\n",
    "top_y = y_test[probs > threshold]\n",
    "top_age = age_test[probs > threshold]\n",
    "# Take a look at average number of likes\n",
    "print(\"AVG number of likes of targeted people:\", top_x.sum(axis=1).mean())\n",
    "print(\"Median number of likes of targeted people:\",np.median(np.array(top_x.sum(axis=1))))\n",
    "# Keep interesting errors\n",
    "top_x = top_x[top_age >= 30]\n",
    "top_y = top_y[top_age >= 30]\n",
    "top_age = top_age[top_age >= 30]\n",
    "top_x = top_x[top_age <= 40]\n",
    "top_y = top_y[top_age <= 40]\n",
    "top_age = top_age[top_age <= 40]\n",
    "print(\"Misclassified individuals between 30 and 40 years:\", top_x.shape[0])\n",
    "# Take a look at probabilities\n",
    "print(\"Probabilities of misclassified individuals:\")\n",
    "print(model.predict_proba(top_x)[:, 1])\n",
    "print(\"Number of likes of misclassified individuals:\")\n",
    "print(top_x.sum(axis=1).reshape(1, -1))\n",
    "order = np.array(top_x.sum(axis=1)).reshape(-1).argsort()[::-1]\n",
    "top_x = top_x[order]\n",
    "top_y = top_y[order]\n",
    "top_age = top_age[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of likes of Percentile 67:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of likes of Percentile 67:\")\n",
    "np.percentile(np.array(X_test[probs > threshold, :].sum(axis=1)).flatten(), 67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Misclassified Individual Index:  37\n",
      "Number of likes: 64.0\n",
      "Age: 34.0\n",
      "Probability of being old: 0.41962539207798977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 3 SHAP VALUES, 5 runs\n",
      "like_id\n",
      "7284978791       ELVIS PRESLEY\n",
      "182736663312    Paul McCartney\n",
      "21931600316         Neil Young\n",
      "Name: name, dtype: object\n",
      "[0.15816004 0.14952443 0.14929379]\n",
      "like_id\n",
      "182736663312    Paul McCartney\n",
      "21931600316         Neil Young\n",
      "55555550744     Brain Pickings\n",
      "Name: name, dtype: object\n",
      "[0.15839785 0.1552504  0.14245585]\n",
      "like_id\n",
      "182736663312    Paul McCartney\n",
      "21931600316         Neil Young\n",
      "55555550744     Brain Pickings\n",
      "Name: name, dtype: object\n",
      "[0.16662192 0.15264056 0.14765539]\n",
      "like_id\n",
      "182736663312    Paul McCartney\n",
      "38026784643      Leonard Cohen\n",
      "7284978791       ELVIS PRESLEY\n",
      "Name: name, dtype: object\n",
      "[0.15766498 0.15552602 0.14593034]\n",
      "like_id\n",
      "182736663312       Paul McCartney\n",
      "7401700249      Bruce Springsteen\n",
      "7284978791          ELVIS PRESLEY\n",
      "Name: name, dtype: object\n",
      "[0.17209662 0.14778275 0.1358186 ]\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# Candidates 12, 3, 15, 21\n",
    "# Candidates 32, 74, 88, 39, 41, 46\n",
    "\n",
    "def make_decision(data):\n",
    "    return (model.predict_proba(data)[:, 1] > threshold).astype(float)\n",
    "\n",
    "#for i in range(30, 99):\n",
    "# Choose arbitrary individual for the analysis\n",
    "i = 37\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Misclassified Individual Index: \", i)\n",
    "top_x_guy = top_x[np.full(5, i), :]\n",
    "print(\"Number of likes:\", top_x_guy[0, :].sum())\n",
    "print(\"Age:\", top_age[i])\n",
    "print(\"Probability of being old:\", model.predict_proba(top_x_guy)[0, 1])\n",
    "np.random.seed(0)\n",
    "default = csr_matrix(np.zeros(X_test.shape[1]))\n",
    "shap_explainer = shap.KernelExplainer(make_decision, default)\n",
    "shap_values = shap_explainer.shap_values(top_x_guy, nsamples=4100, l1_reg='aic')\n",
    "\n",
    "print(\"TOP 3 SHAP VALUES, 5 runs\")\n",
    "# Top 3 predictive features are different every time. \n",
    "for i in range(shap_values.shape[0]):\n",
    "    i_vals = (-shap_values[i, :]).argsort()\n",
    "    print(df_names.loc[pages_c[i_vals[:3]]].name)\n",
    "    print(shap_values[i, i_vals[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to find counterfactual explanations for individual above\n",
      "0.07978582382202148\n",
      "Explanations found\n",
      "33\n",
      "Length of explanation\n",
      "1\n",
      "First explanations\n",
      "                        name\n",
      "like_id                     \n",
      "182736663312  Paul McCartney\n",
      "                     name\n",
      "like_id                  \n",
      "7284978791  ELVIS PRESLEY\n",
      "                   name\n",
      "like_id                \n",
      "21931600316  Neil Young\n",
      "                      name\n",
      "like_id                   \n",
      "38026784643  Leonard Cohen\n",
      "                       name\n",
      "like_id                    \n",
      "55555550744  Brain Pickings\n"
     ]
    }
   ],
   "source": [
    "# See how long it takes for our method to find one explanation\n",
    "import explainer\n",
    "import time\n",
    "\n",
    "def scoring_function(data):\n",
    "    return model.predict_proba(data)[:, 1]\n",
    "\n",
    "# Get evidence-based explanations\n",
    "explain = explainer.Explainer(scoring_function, default, prune=False)\n",
    "t0 = time.time()\n",
    "explanations = explain.explain(top_x_guy[0, :], thresholds=float(threshold), max_ite=50)\n",
    "t1 = time.time()\n",
    "print(f\"Time to find counterfactual explanations for individual above\")\n",
    "print(t1-t0)\n",
    "print(\"Explanations found\")\n",
    "print(len(explanations[0]))\n",
    "print(\"Length of explanation\")\n",
    "print(len(explanations[0][0]))\n",
    "print(\"First explanations\")\n",
    "print(df_names.loc[pages_c[explanations[0][0]]])\n",
    "print(df_names.loc[pages_c[explanations[0][1]]])\n",
    "print(df_names.loc[pages_c[explanations[0][2]]])\n",
    "print(df_names.loc[pages_c[explanations[0][3]]])\n",
    "print(df_names.loc[pages_c[explanations[0][4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 3, 1, 3, 2, 2, 2, 0, 2, 3, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 1, 2, 3, 2, 3, 3, 2, 3, 0, 3, 3, 1, 1, 1, 0, 0, 3, 1, 1, 1, 2, 3, 3, 2, 2, 1, 2, 2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 0, 1, 3, 2, 3, 1, 3, 3, 3, 1, 2, 2, 1, 2, 2, 2, 3, 3, 2, 1, 3, 2, 3, 1, 3, 1, 3, 3, 2, 0, 3, 3, 3, 2, 1, 3, 0, 2, 1, 3, 2, 2, 3, 3, 2, 0, 3, 3, 3, 2, 0, 2, 2, 1, 3, 2, 2, 3, 0, 0, 3, 3, 1, 3, 1, 2, 2, 2, 3, 1, 1, 2, 1, 3, 3, 2, 0, 2, 3, 1, 1, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 2, 1, 0, 2, 1, 1, 3, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 0, 2, 2, 2, 2, 1, 3, 3, 2, 2, 1, 2, 3, 2, 3, 3, 3, 3, 3, 3, 2, 0, 0, 3, 3, 2, 1, 3, 1, 2, 1, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 1, 3, 3, 1, 2, 2, 3, 1, 1, 2, 2, 0, 3, 1, 1, 2, 3, 2, 3, 3, 2, 2, 0, 3, 2, 3, 1, 3, 3, 2, 3, 2, 2, 1, 3, 2, 3, 3, 1, 1, 1, 1, 0, 2, 1, 3, 0, 3, 0, 3, 1, 2, 3, 2, 1, 1, 3, 3, 2, 1, 3, 3, 2, 3, 0, 2, 2, 2, 3, 3, 3, 3, 3, 0, 2, 1, 2, 1, 3, 2, 2, 3, 0, 3, 2, 0, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 3, 3, 0, 3, 2, 2, 1, 1, 0, 3, 2, 1, 2, 3, 1, 1, 2, 2, 2, 3, 1, 3, 2, 1, 1, 2, 1, 1, 3, 1, 0, 3, 3, 1, 2, 0, 2, 3, 2, 1, 0, 0, 3, 1, 2, 1, 3, 3, 1, 2, 3, 0, 1, 3, 2, 3, 2, 1, 1, 1, 2, 1, 2, 2, 3, 2, 1, 1, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 1, 3, 3, 3, 2, 3, 2, 3, 2, 2, 2, 1, 2, 2, 1, 1, 0, 2, 3, 1, 3, 3, 2, 2, 3, 2, 2, 2, 1, 1, 2, 2, 3, 0, 2, 1, 1, 2, 2, 1, 2, 3, 3, 3, 3, 1, 3, 3, 1, 0, 3, 1, 3, 2, 3, 2, 1, 0, 1, 2, 3, 3, 3, 1, 3, 1, 3, 3, 1, 3, 2, 2, 3, 3, 2, 3, 2, 1, 3, 2, 2, 1, 2, 3, 3, 3, 2, 2, 1, 1, 1, 2, 2, 3, 2, 0, 1, 0, 2, 3, 2, 3, 2, 2, 1, 3, 2, 2, 1, 0, 1, 3, 3, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "sample_size = 500\n",
    "np.random.seed(0)\n",
    "# Get sample of targeted observations to explain\n",
    "X_targeted = X_test[probs > threshold, :]\n",
    "X_sample = X_targeted[np.random.choice(X_targeted.shape[0], sample_size, replace=False), :]\n",
    "X_to_explain = X_sample[np.arange(X_sample.shape[0]).repeat(5), :]\n",
    "# Obtain their SHAP values (commented because it takes too long)\n",
    "#shap_values = shap_explainer.shap_values(X_to_explain, nsamples=4100, l1_reg='aic')\n",
    "#np.save(\"Data/back_up_shap_values.npy\", shap_values)\n",
    "shap_values = np.load(\"Data/back_up_shap_values.npy\")\n",
    "# Find top 5 most informative features\n",
    "most_informative_f = np.argsort(shap_values, axis=1)[:, -3:]\n",
    "# Compute number of matches\n",
    "sample_matches = list()\n",
    "for i in range(sample_size):\n",
    "    matches = reduce(np.intersect1d, most_informative_f[(i*5):((i+1)*5), :]).size\n",
    "    #matches = np.unique(most_informative_f[(i*5):((i+1)*5), :]).size\n",
    "    sample_matches.append(matches)\n",
    "    \n",
    "print(sample_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper limit:  18.0\n",
      "2.7065217391304346\n",
      "Upper limit:  34.0\n",
      "2.2547169811320753\n",
      "Upper limit:  58.39999999999998\n",
      "2.1372549019607843\n",
      "Upper limit:  99.0\n",
      "1.7142857142857142\n",
      "Upper limit:  inf\n",
      "1.1470588235294117\n"
     ]
    }
   ],
   "source": [
    "likes_count = np.array(X_sample.sum(axis=1)).flatten()\n",
    "cut_points = list()\n",
    "for i in range(1, 5):\n",
    "    cut_points.append(np.percentile(likes_count, i*20))\n",
    "cut_points.append(np.inf)\n",
    "\n",
    "min_likes = 0\n",
    "sample_matches = np.array(sample_matches) \n",
    "for cp in cut_points:\n",
    "    print(\"Upper limit: \", cp)\n",
    "    selected_obs = (likes_count >= min_likes) & (likes_count < cp)\n",
    "    print(sample_matches[selected_obs].mean())\n",
    "    min_likes = cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to find first explanation for 500 instances\n",
      "14.732604742050171\n",
      "Upper limit:  18.0\n",
      "1.173913043478261\n",
      "Upper limit:  34.0\n",
      "2.0\n",
      "Upper limit:  58.39999999999998\n",
      "2.303921568627451\n",
      "Upper limit:  99.0\n",
      "3.6020408163265305\n",
      "Upper limit:  inf\n",
      "4.901960784313726\n"
     ]
    }
   ],
   "source": [
    "# Get counterfactual explanations\n",
    "explainer.Explainer(scoring_function, default, prune=False)\n",
    "\n",
    "t0 = time.time()\n",
    "sample_e = explain.explain(X_sample, thresholds=float(threshold), max_ite=50, stop_at_first=True)\n",
    "t1 = time.time()\n",
    "print(\"Time to find first explanation for 500 instances\")\n",
    "print(t1-t0)\n",
    "\n",
    "sample_lens = np.array([len(e[0]) for e in sample_e])\n",
    "min_likes = 0\n",
    "for cp in cut_points:\n",
    "    print(\"Upper limit: \", cp)\n",
    "    selected_obs = (likes_count >= min_likes) & (likes_count < cp)\n",
    "    print(sample_lens[selected_obs].mean())\n",
    "    min_likes = cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0\n",
      "33.0\n",
      "53.0\n",
      "94.0\n"
     ]
    }
   ],
   "source": [
    "# Check out real quantiles\n",
    "likes_count = X_targeted.sum(axis=1)\n",
    "\n",
    "print(np.percentile(likes_count, 20))\n",
    "print(np.percentile(likes_count, 40))\n",
    "print(np.percentile(likes_count, 60))\n",
    "print(np.percentile(likes_count, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:  11\n",
      "Explanation WITHOUT costs\n",
      "It's a Wonderful Life | Likes : 1181\n",
      "JESUS IS LORD!!!!!!!!!!!!!!!!!!!!!!!!!!! if you know this is true press like. :) | Likes : 1291\n",
      "---\n",
      "Explanation WITH costs\n",
      "Reading | Likes : 47288\n",
      "American Idol | Likes : 15792\n",
      "Classical | Likes : 8632\n",
      "*************************************************\n",
      "ID:  38\n",
      "Explanation WITHOUT costs\n",
      "The Hollywood Gossip | Likes : 1353\n",
      "Let's remember those who have passed. Press Like if you've lost a loved one... | Likes : 2248\n",
      "---\n",
      "Explanation WITH costs\n",
      "Pink Floyd | Likes : 43045\n",
      "Dancing With The Stars | Likes : 5379\n",
      "The Ellen DeGeneres Show | Likes : 16944\n",
      "American Idol | Likes : 15792\n",
      "*************************************************\n",
      "ID:  108\n",
      "Explanation WITHOUT costs\n",
      "Six Degrees Of Separation - The Experiment | Likes : 1275\n",
      "They're, Their, and There have 3 distinct meanings. Learn Them. | Likes : 3842\n",
      "---\n",
      "Explanation WITH costs\n",
      "Star Trek | Likes : 11683\n",
      "Turn Facebook Pink For 1 Week For Breast Cancer Awareness | Likes : 12942\n",
      "*************************************************\n",
      "ID:  413\n",
      "Explanation WITHOUT costs\n",
      "Sarcasm as a second language | Likes : 1540\n",
      "RightChange | Likes : 2427\n",
      "---\n",
      "Explanation WITH costs\n",
      "Reading | Likes : 47288\n",
      "Pink Floyd | Likes : 43045\n",
      "Where the Wild Things Are | Likes : 13781\n",
      "Proud to be an American | Likes : 3938\n",
      "*************************************************\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Define Cost Function\n",
    "likes_per_page = np.array(sparse_mat.sum(axis=0)).reshape(-1)\n",
    "costs = norm.cdf((np.mean(likes_per_page) - likes_per_page) / np.std(likes_per_page))\n",
    "costs += np.full(X_train.shape[1], 0.1)\n",
    "\n",
    "def likes_cost_func(original_obs, new_obs):\n",
    "    return (original_obs - new_obs).dot(costs)\n",
    "\n",
    "# Method to print explanation details\n",
    "def print_explanation(e):\n",
    "    for p in e:\n",
    "        print(df_names.loc[pages_c[p]].values[0], \"| Likes :\", int(likes_per_page[p]))\n",
    "\n",
    "#  Find explanations with/without costs\n",
    "ixs = [11, 38, 108, 413]\n",
    "t = float(threshold)\n",
    "for i in ixs:#range(138, 500):\n",
    "    # With cost\n",
    "    cost_e = explain.explain(X_sample[i, :], thresholds=t, max_ite=300, stop_at_first=True,\n",
    "                             cost_func=likes_cost_func)[0]\n",
    "    cost_e = cost_e[0] if len(cost_e) > 0 else []\n",
    "    # No cost\n",
    "    no_cost_e = explain.explain(X_sample[i, :], thresholds=t, max_ite=300, stop_at_first=True)[0][0]\n",
    "    print(\"ID: \", i)\n",
    "    print(\"Explanation WITHOUT costs\")\n",
    "    print_explanation(no_cost_e)\n",
    "    print(\"---\")\n",
    "    print(\"Explanation WITH costs\")\n",
    "    print_explanation(cost_e)\n",
    "    print(\"*************************************************\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
